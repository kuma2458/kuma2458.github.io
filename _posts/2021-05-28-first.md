---
title:  "과제6 랜덤포레스트 및 스태킹 구현"
---

+ [기본설정](#----)

- [1.랜덤 포레스트 모델 구현](#1-------------)
    + [데이터세트 준비](#--------)
    + [결정트리 모델 준비](#----------)
    + [랜덤포레스트 모델 구현](#------------)
- [스태킹 모델 구현](#---------)
    + [데이터 준비](#------)
    + [분류기 제작](#------)
    + [9. Stacking Ensemble](#9-stacking-ensemble)
    + [사이킷런의 `StackingClassifier` 모델](#-------stackingclassifier----)

### 기본설정

```python
# Python ≥3.5 is required
import sys
assert sys.version_info >= (3, 5)

# Scikit-Learn ≥0.20 is required
import sklearn
assert sklearn.__version__ >= "0.20"

# Common imports
import numpy as np
import os

# to make this notebook's output stable across runs
np.random.seed(42)

# To plot pretty figures
%matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)
​```



# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "decision_trees"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)
os.makedirs(IMAGES_PATH, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)

from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

iris = load_iris()
X = iris.data[:, 2:] # petal length and width
y = iris.target

tree_clf = DecisionTreeClassifier(max_depth=2, random_state=42)
tree_clf.fit(X, y)
```

# 1.랜덤 포레스트 모델 구현

### 데이터세트 준비

*   `make_moons(n_samples=500, noise=0.30)` 을 사용해 데이터셋을 생성합니다.

`random_state=42`를 추가해 동일한 결과가 나오도록 합니다.

```python
from sklearn.datasets import make_moons

X, y = make_moons(n_samples=500, noise=0.30, random_state=42)
```

*   `train_test_split()`을 사용해 훈련 세트와 테스트 세트로 나눕니다.

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

### 결정트리 모델 준비

*  `DecisionTreeClassifier`의 최적의 매개변수를 찾기 위해 교차 검증과 함께 그리드 탐색을 수햅합니다(`GridSearchCV`를 사용하면 됩니다).



```python
from sklearn.model_selection import GridSearchCV

params = {'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4]}
grid_search_cv = GridSearchCV(DecisionTreeClassifier(random_state=42), params, verbose=1, cv=3)

grid_search_cv.fit(X_train, y_train)

grid_search_cv.best_estimator_
```

```python
=>Fitting 3 folds for each of 294 candidates, totalling 882 fits
[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done 882 out of 882 | elapsed:    1.3s finished
GridSearchCV(cv=3, error_score=nan,
             estimator=DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,
                                              criterion='gini', max_depth=None,
                                              max_features=None,
                                              max_leaf_nodes=None,
                                              min_impurity_decrease=0.0,
                                              min_impurity_split=None,
                                              min_samples_leaf=1,
                                              min_samples_split=2,
                                              min_weight_fraction_leaf=0.0,
                                              presort='deprecated',
                                              random_state=42,
                                              splitter='best'),
             iid='deprecated', n_jobs=None,
             param_grid={'max_leaf_nodes': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,
                                            13, 14, 15, 16, 17, 18, 19, 20, 21,
                                            22, 23, 24, 25, 26, 27, 28, 29, 30,
                                            31, ...],
                         'min_samples_split': [2, 3, 4]},
             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,
             scoring=None, verbose=1)
```

- 최적의 Max_leaf_nodes는 14로 나왔습니다.


*  찾은 매개변수를 사용해 전체 훈련 세트에 대해 모델을 훈련시키고 테스트 세트에서 성능을 측정합니다. 



기본적으로 `GridSearchCV`는 전체 교육 세트에 있는 최상의 모델을 교육하므로(`Refet=False`를 설정하여 변경할 수 있음) 다시 교육할 필요가 없습니다. 모델의 정확도를 간단하게 평가할 수 있습니다.

```python
from sklearn.metrics import accuracy_score

y_pred = grid_search_cv.predict(X_test)
accuracy_score(y_test, y_pred)
```

```python
=>	0.9
```

- 결정트리 모델의 정확도가 90%가 나온것을 알 수 있습니다.



### 랜덤포레스트 모델 구현

- 사이킷런의 `ShuffleSplit`을 사용해 나눴습니다.

```python
from sklearn.model_selection import ShuffleSplit

n_trees = 500 # 7장 랜덤포레스트 모델과 비교하기 위해 결정트리 개수를 500개로 통일함
n_instances = 100

mini_sets = []

rs = ShuffleSplit(n_splits=n_trees, test_size=len(X_train) - n_instances, random_state=42)
for mini_train_index, mini_test_index in rs.split(X_train):
    X_mini_train = X_train[mini_train_index]
    y_mini_train = y_train[mini_train_index]
    mini_sets.append((X_mini_train, y_mini_train))
```

* 이전 연습문제에서 찾은 최적의 매개변수를 사용해 각 서브셋에 결정 트리를 훈련시킵니다. 

* 최적의 매개변수는 Max_leaf_nodes = 14로 나왔습니다.

* 테스트 세트로 이 500개의 결정 트리를 평가합니다. 

  

```python
from sklearn.base import clone

forest = [clone(grid_search_cv.best_estimator_) for _ in range(n_trees)]

accuracy_scores = []

for tree, (X_mini_train, y_mini_train) in zip(forest, mini_sets):
    tree.fit(X_mini_train, y_mini_train) 
    
    y_pred = tree.predict(X_test)
accuracy_scores.append(accuracy_score(y_test, y_pred))

np.mean(accuracy_scores)
```

    =>	0.8496600000000001



*   랜덤포레스트를 구현하기 위해 만든 결정트리의 정확도보다 더 낮아졌습니다.
*   추가적으로 수정해줘야 합니다.

*   각 테스트 세트 샘플에 대해 500개의 결정 트리 예측을 만들고 다수로 나온 예측만 취합니다.(사이파이의 `mode()` 함수를 사용할 수 있습니다). 
*   그러면 테스트 세트에 대한 다수결 예측이 만들어집니다. 

```python
Y_pred = np.empty([n_trees, len(X_test)], dtype=np.uint8)

for tree_index, tree in enumerate(forest):
    Y_pred[tree_index] = tree.predict(X_test)

from scipy.stats import mode

y_pred_majority_votes, n_votes = mode(Y_pred, axis=0)
```

*   테스트 세트에서 이 예측을 평가합니다.
*   이전 정확도보다 0.5%가 오른걸 볼 수 있음

```python
accuracy_score(y_test, y_pred_majority_votes.reshape([-1]))
```

```python
=>	0.89
```

```python
from sklearn.ensemble import RandomForestClassifier

rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, random_state=42)
rnd_clf.fit(X_train, y_train)

y_pred_rf = rnd_clf.predict(X_test)
```

```python
np.sum(y_pred == y_pred_rf) / len(y_pred) # almost identical predictions
```

```python
=>	0.92
```

- 7장의 랜덤포레스트 분류 모델과 직접 만든 랜덤 포레스트 모델과 비교한 결과 0.3%정도의 차이를 보인다.

  -이정도의 차이는 크지않아 비슷하다고 판별된다.

# 스태킹 모델 구현

### 데이터 준비

- 데이터 샘플을 이리스 붓꽃 데이터를 사용하였습니다

```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

X, y = load_iris(return_X_y=True)

X_train_val, X_test, y_train_val, y_test = train_test_split(
    X, y, stratify=y, random_state=42)

X_train, X_val, y_train, y_val = train_test_split(
    X_train_val, y_train_val, test_size=0.2, random_state=42)
```



### 분류기 제작

- 랜덤 포레스트 분류기, 엑스트라 트리 분류기, SVM 분류기 등 3종류의 분류기를 훈련시킵니다.

```python
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier
from sklearn.svm import LinearSVC
from sklearn.neural_network import MLPClassifier
```

```python
random_forest_clf = RandomForestClassifier(n_estimators=100, random_state=42)
extra_trees_clf = ExtraTreesClassifier(n_estimators=100, random_state=42)
svm_clf = LinearSVC(max_iter=100, tol=20, random_state=42)
mlp_clf = MLPClassifier(random_state=42)

```

```py
estimators = [random_forest_clf, extra_trees_clf, svm_clf, mlp_clf]
for estimator in estimators:
    print("Training the", estimator)
    estimator.fit(X_train, y_train)
```

```python
=>	Training the RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=100,
                       n_jobs=None, oob_score=False, random_state=42, verbose=0,
                       warm_start=False)
Training the ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='auto',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_impurity_split=None,
                     min_samples_leaf=1, min_samples_split=2,
                     min_weight_fraction_leaf=0.0, n_estimators=100,
                     n_jobs=None, oob_score=False, random_state=42, verbose=0,
                     warm_start=False)
Training the LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
          intercept_scaling=1, loss='squared_hinge', max_iter=100,
          multi_class='ovr', penalty='l2', random_state=42, tol=20, verbose=0)
Training the MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
              beta_2=0.999, early_stopping=False, epsilon=1e-08,
              hidden_layer_sizes=(100,), learning_rate='constant',
              learning_rate_init=0.001, max_fun=15000, max_iter=200,
              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,
              power_t=0.5, random_state=42, shuffle=True, solver='adam',
              tol=0.0001, validation_fraction=0.1, verbose=False,
              warm_start=False)
/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  % self.max_iter, ConvergenceWarning)
```

```python
[estimator.score(X_val, y_val) for estimator in estimators]
```

```python
=>	[0.9565217391304348, 0.9565217391304348, 0.6086956521739131, 1.0]
```



선형 SVM은 다른 분류기에 비해 성능이 월등합니다. 하지만, 투표 분류기의 성능이 향상될 수 있으니 일단은 유지합시다.

_다음, 소프트 또는 하드 투표 분류기를 사용하여 검증 세트에서 모든 것을 능가하는 앙상블에 그것들을 결합하려고 시도한다._

```python
from sklearn.ensemble import VotingClassifier
```

```python
named_estimators = [
    ("random_forest_clf", random_forest_clf),
    ("extra_trees_clf", extra_trees_clf),
    ("svm_clf", svm_clf),
    ("mlp_clf", mlp_clf),
]
```

```python
voting_clf = VotingClassifier(named_estimators)
```

```python
voting_clf.fit(X_train, y_train)
```

```python
=>	/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.
  % self.max_iter, ConvergenceWarning)
VotingClassifier(estimators=[('random_forest_clf',
                              RandomForestClassifier(bootstrap=True,
                                                     ccp_alpha=0.0,
                                                     class_weight=None,
                                                     criterion='gini',
                                                     max_depth=None,
                                                     max_features='auto',
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=0.0,
                                                     min_impurity_split=None,
                                                     min_samples_leaf=1,
                                                     min_samples_split=2,
                                                     min_weight_fraction_leaf=0.0,
                                                     n_estimators=100,
                                                     n_jobs...
                                            epsilon=1e-08,
                                            hidden_layer_sizes=(100,),
                                            learning_rate='constant',
                                            learning_rate_init=0.001,
                                            max_fun=15000, max_iter=200,
                                            momentum=0.9, n_iter_no_change=10,
                                            nesterovs_momentum=True,
                                            power_t=0.5, random_state=42,
                                            shuffle=True, solver='adam',
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=False, warm_start=False))],
                 flatten_transform=True, n_jobs=None, voting='hard',
                 weights=None)
```

```python
voting_clf.score(X_val, y_val)
```

```python
=>	0.9565217391304348
```

```python
[estimator.score(X_val, y_val) for estimator in voting_clf.estimators_]
```

```python
=>	[0.9565217391304348, 0.9565217391304348, 0.6086956521739131, 1.0]
```



- SVM을 제거하여 성능이 향상되는지 확인해 보겠습니다. 다음과 같이 set_params()를 사용하여 'None'으로 설정하여 추정기를 제거할 수 있다.

```python
voting_clf.set_params(svm_clf=None)
```

```python
=>	VotingClassifier(estimators=[('random_forest_clf',
                              RandomForestClassifier(bootstrap=True,
                                                     ccp_alpha=0.0,
                                                     class_weight=None,
                                                     criterion='gini',
                                                     max_depth=None,
                                                     max_features='auto',
                                                     max_leaf_nodes=None,
                                                     max_samples=None,
                                                     min_impurity_decrease=0.0,
                                                     min_impurity_split=None,
                                                     min_samples_leaf=1,
                                                     min_samples_split=2,
                                                     min_weight_fraction_leaf=0.0,
                                                     n_estimators=100,
                                                     n_jobs...
                                            epsilon=1e-08,
                                            hidden_layer_sizes=(100,),
                                            learning_rate='constant',
                                            learning_rate_init=0.001,
                                            max_fun=15000, max_iter=200,
                                            momentum=0.9, n_iter_no_change=10,
                                            nesterovs_momentum=True,
                                            power_t=0.5, random_state=42,
                                            shuffle=True, solver='adam',
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=False, warm_start=False))],
                 flatten_transform=True, n_jobs=None, voting='hard',
                 weights=None)
```

- 이렇게 하면 추정기 목록이 업데이트됩니다.

```python
voting_clf.estimators
```

```python
=>	[('random_forest_clf',
  RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                         criterion='gini', max_depth=None, max_features='auto',
                         max_leaf_nodes=None, max_samples=None,
                         min_impurity_decrease=0.0, min_impurity_split=None,
                         min_samples_leaf=1, min_samples_split=2,
                         min_weight_fraction_leaf=0.0, n_estimators=100,
                         n_jobs=None, oob_score=False, random_state=42, verbose=0,
                         warm_start=False)),
 ('extra_trees_clf',
  ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=100,
                       n_jobs=None, oob_score=False, random_state=42, verbose=0,
                       warm_start=False)),
 ('svm_clf', None),
 ('mlp_clf',
  MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
                beta_2=0.999, early_stopping=False, epsilon=1e-08,
                hidden_layer_sizes=(100,), learning_rate='constant',
                learning_rate_init=0.001, max_fun=15000, max_iter=200,
                momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,
                power_t=0.5, random_state=42, shuffle=True, solver='adam',
                tol=0.0001, validatio
```



- 그러나 훈련된 추정기 목록은 업데이트가 아직 안됐습니다.

```python
voting_clf.estimators_
```

```python
[RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                        criterion='gini', max_depth=None, max_features='auto',
                        max_leaf_nodes=None, max_samples=None,
                        min_impurity_decrease=0.0, min_impurity_split=None,
                        min_samples_leaf=1, min_samples_split=2,
                        min_weight_fraction_leaf=0.0, n_estimators=100,
                        n_jobs=None, oob_score=False, random_state=42, verbose=0,
                        warm_start=False),
 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                      criterion='gini', max_depth=None, max_features='auto',
                      max_leaf_nodes=None, max_samples=None,
                      min_impurity_decrease=0.0, min_impurity_split=None,
                      min_samples_leaf=1, min_samples_split=2,
                      min_weight_fraction_leaf=0.0, n_estimators=100,
                      n_jobs=None, oob_score=False, random_state=42, verbose=0,
                      warm_start=False),
 LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,
           intercept_scaling=1, loss='squared_hinge', max_iter=100,
           multi_class='ovr', penalty='l2', random_state=42, tol=20, verbose=0),
 MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
               beta_2=0.999, early_stopping=False, epsilon=1e-08,
               hidden_layer_sizes=(100,), learning_rate='constant',
               learning_rate_init=0.001, max_fun=15000, max_iter=200,
               momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,
               power_t=0.5, random_state=42, shuffle=True, solver='adam',
               tol=0.0001, validation_fraction=0.1, verbose=False,
               warm_start=False)]
```

- 따라서 `BotingClassifier`를 다시 장착하거나 훈련된 추정기 목록에서 SVM을 제거할 수 있습니다.

```python
del voting_clf.estimators_[2]
```

- 이제 `BotingClassifier`를 다시 한 번 평가해 보겠습니다

```python
voting_clf.score(X_val, y_val)
```

```python
=>	0.9565217391304348
```

- 아주 조금 더 나아졌네요! SVM은 성능을 저하시켰었습니다. 이번엔 soft voting을 사용해 보도록 합시다.우리는 선형 분급기를 재교육할때, 우리는 `soft`로 `voting` 설정할 수 있습니다.

```python
voting_clf.voting = "soft"
```

```python
voting_clf.score(X_val, y_val)
```

```python
=>	0.9565217391304348
```

- `hard voting`이 성능이 더 낮아졌다.

```python
voting_clf.voting = "hard"
```

```python
voting_clf.score(X_test, y_test)
```

```python
=>	0.9210526315789473
```



```python
[estimator.score(X_test, y_test) for estimator in voting_clf.estimators_]
```

```python
=>	[0.8947368421052632, 0.9210526315789473, 0.9736842105263158]
```

투표 분류기는 이 경우에 가장 좋은 모델의 오류율을 아주 조금 줄였을 뿐이다.

### 9. Stacking Ensemble

- 이전 연습의 개별 분류기를 실행하여 검증 세트에 대한 예측을 하고 결과 예측으로 새로운 훈련 세트를 만듭니다. 각 훈련 인스턴스는 이미지에 대한 모든 분류기의 예측 집합을 포함하는 벡터이며 대상은 이미지의 클래스입니다. 이 새 훈련 세트에 대한 분류기를 교육합니다._

```python
X_val_predictions = np.empty((len(X_val), len(estimators)), dtype=np.float32)

for index, estimator in enumerate(estimators):
    X_val_predictions[:, index] = estimator.predict(X_val)
```

```python
X_val_predictions
```

```python
=>	array([[0., 0., 0., 0.],
       [1., 1., 2., 1.],
       [1., 1., 2., 1.],
       [0., 0., 0., 0.],
       [2., 2., 2., 2.],
       [2., 2., 2., 2.],
       [2., 2., 2., 2.],
       [2., 2., 2., 2.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [0., 0., 0., 0.],
       [2., 2., 2., 2.],
       [1., 1., 2., 1.],
       [1., 1., 2., 1.],
       [2., 2., 2., 2.],
       [0., 0., 0., 0.],
       [1., 1., 2., 1.],
       [1., 1., 2., 1.],
       [1., 1., 2., 2.],
       [0., 0., 0., 0.],
       [1., 1., 2., 1.],
       [1., 1., 2., 1.],
       [1., 1., 2., 1.]], dtype=float32)
```

```python
rnd_forest_blender = RandomForestClassifier(n_estimators=200, oob_score=True, random_state=42)
rnd_forest_blender.fit(X_val_predictions, y_val)
```

```python
=>	RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=200,
                       n_jobs=None, oob_score=True, random_state=42, verbose=0,
                       warm_start=False)
```

```python
rnd_forest_blender.oob_score_
```

```python
=>	0.9565217391304348
```



- 이 블렌더를 미세 조정하거나 다른 유형의 블렌더(예: `MLPClassifier`)를 사용한 후 항상 그렇듯이 교차 검증을 사용하여 최적의 블렌더를 선택할 수 있습니다.
- 축하합니다. 방금 블렌더를 훈련시켰습니다. 그리고 이 분류기를 모아서 스태킹 앙상블을 구성했습니다! <br>이제 테스트 세트의 앙상블을 평가해보세요. <br>테스트 세트의 각 이미지에 대해 모든 분류기로 예측을 만들고 앙상블의 예측 결과를 만들기 위해 블렌더에 그 예측을 주입합니다. 앞서 교육한 투표 분류기와 비교했을 때 어떤가요?



```python
X_test_predictions = np.empty((len(X_test), len(estimators)), dtype=np.float32)

for index, estimator in enumerate(estimators):
    X_test_predictions[:, index] = estimator.predict(X_test)
```

```python
y_pred = rnd_forest_blender.predict(X_test_predictions)

```python
from sklearn.metrics import accuracy_score
```

```python
accuracy_score(y_test, y_pred)
```

```python
=>	0.9473684210526315
```

- 스태킹 앙상블을 구현하기 위해 만든 분류기보다 성능이 떨어진 것을 확인 할 수 있다.

### 사이킷런의 `StackingClassifier` 모델

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.ensemble import StackingClassifier
```



```python
estimators = [('rf', RandomForestClassifier(n_estimators=10, random_state=42)),
              ('svr', make_pipeline(StandardScaler(),
                                    LinearSVC(random_state=42)))]

clf = StackingClassifier(estimators=estimators, 
                         final_estimator=LogisticRegression())
```

```python
clf.fit(X_train, y_train).score(X_test, y_test)
```

```python
=>	0.9210526315789473
```

> ​	직접만든 스태킹 모델이 성능이 조금 더 좋은게 보이지만 크게 차이가 나지 않는다.

