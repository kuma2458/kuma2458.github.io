---
layout: post
title:  "경사하강법을 이용한 로지스틱 회귀 이진분류 모델 구현 및 응용"
---



프로젝트 설명
===========

붓꽃(iris) 데이터를 sklearn의 모델을 사용하지않고 이진분류하기 위해 직접 경사하강법을 이용한 로지스틱 회귀를 만들고 이를 응용하는 과제다.

기본 설정
======
 - 필수 모듈 불러오기
 - 그래프 출력 관련 기본 설정 지정

> ###### 로지스틱 분류를 만들기 이전에 기본적으로 설정해야될 요소들을 코드로 작성했다.

파이썬 ≥3.5 설정
---------

```python
import sys
assert sys.version_info >= (3, 5)
```

사이킷런 ≥0.20 설정
-----
```python
import sklearn
assert sklearn.__version__ >= "0.20"
```

> ###### sklearn 모델을 사용하지 않는다 했지만 iris 자료를 다운받기위해 사용했다.

공통 모듈 임포트
-------

```python
import numpy as np
import os
```
노트북 실행 결과를 동일하게 유지하기 위해 random.seed 설정
-------
```python
np.random.seed(42)
```

iris 데이터 다운로드
---------
```python

from sklearn import datasets
iris = datasets.load_iris()
```





# 과제 1

- 조기 종료를 사용한 배치 경사 하강법으로 로지스틱 회귀를 구현하라.
- 단, 사이킷런을 전혀 사용하지 않아야 한다.



> ###### 이 과제를 처음 봤을때 조금 난감했다. <br>내가 직접 모델을 만들라고?  어떻게?<br>그래도 이전파트에 있는 코드를 이용하면 된다고 하셨기에 천천히 살펴봤었다.



__1. 데이터 준비__
---
```
X = iris["data"][:, 2:]  #꽃잎 길이
y = (iris["target"]==0).astype(np.int) #세토사를 타겟으로 설정
```

> ###### 일단 이진분류를 하는게 목표이기 때문에 특성과 타겟을 재설정했다.<br>꽃잎 길이 특성이 이진분류하는데 깔끔하게 나올거같아서  사용했다.<br>타겟을 세토사로 한 이유는 훈련 결과가 잘 나오기 때문에 사용...

##### 편향에 특성을 추가

---

```
X_with_bias = np.c_[np.ones([len(X), 1]), X]
```

> ###### 이 코드를 추가한 이유는 추가를 안하고 훈련을 시킨거보다 추가하고 훈련을 한게 점수가 더 잘나왔기 때문이다

##### 결과를 일정하게 유지하기 위해 랜덤 시드를 지정한다.

---

```
np.random.seed(2042)
```



__단계 2: 데이터셋 분할__ 
---
##### 데이터셋을 훈련, 검증, 테스트 용도로 6대 2대 2의 비율로 무작위로 분할한다.

```python
test_ratio = 0.2                                         # 테스트 세트 비율 = 20%
validation_ratio = 0.2                                   # 검증 세트 비율 = 20%
total_size = len(X_with_bias)                            # 전체 데이터셋 크기

test_size = int(total_size * test_ratio)                 # 테스트 세트 크기: 전체의 20%
validation_size = int(total_size * validation_ratio)     # 검증 세트 크기: 전체의 20%
train_size = total_size - test_size - validation_size    # 훈련 세트 크기: 전체의 60%
```



##### `np.random.permutation()` 함수를 이용하여 인덱스를 무작위로 섞는다. 

```
rnd_indices = np.random.permutation(total_size)
```

> ###### 인덱스를 무작위로 섞는 이유는 코드를 반복해서 실행했을때 동일한 결과를 얻기 위해서다.



##### 훈련 세트, 검증 세트, 테스트 세트를 각 6:2:2 비율로 나눈다.

---

```
X_train = X_with_bias[rnd_indices[:train_size]]
y_train = y[rnd_indices[:train_size]]

X_valid = X_with_bias[rnd_indices[train_size:-test_size]]
y_valid = y[rnd_indices[train_size:-test_size]]

X_test = X_with_bias[rnd_indices[-test_size:]]
y_test = y[rnd_indices[-test_size:]]
```

> ###### X는 편향을 추가한 데이터를 6:2:2로 나눴다.



__단계 3: 타깃 변환__ 
---

##### 타깃은 0, 1로 설정되어 있다. 타겟으로 설정한 세토사는 1로, 나머지 버시컬러, 버지니카는 0으로 나온다.
##### 훈련 세트의 첫 5개 샘플의 품종은 다음과 같다.

```python
y_train[:5]
```

```
=> array([0, 0, 1, 0, 0])
```



##### 원-핫 인코딩 사용

```python
def to_one_hot(y):
    n_classes = y.max() + 1                 # 클래스 수
    m = len(y)                              # 샘플 수
    Y_one_hot = np.zeros((m, n_classes))    # (샘플 수, 클래스 수) 0-벡터 생성
    Y_one_hot[np.arange(m), y] = 1          # 샘플 별로 해당 클래스의 값만 1로 변경. (넘파이 인덱싱 활용)
    return Y_one_hot
```

> ###### 원핫인코딩으로 범주형데이터에 대해 비교를할때 클래스에 부여된 숫자가 영향을 주지않기 위해 인코딩을 했다.

```
y_train[:5]
```

```
=> array([0, 0, 1, 0, 0])
```

> ###### 바로 위의  결과값과 변함 없는이유는 원래 0과 1로 나눠서...

```
to_one_hot(y_train[:5])

```
=> array([[1., 0.],
       [1., 0.],
       [0., 1.],
       [1., 0.],
       [1., 0.]])

##### 이제 훈련/검증/테스트 세트의 타깃을 모두 원-핫 벡터로 변환한다.

```python
Y_train_one_hot = to_one_hot(y_train)
Y_valid_one_hot = to_one_hot(y_valid)
Y_test_one_hot = to_one_hot(y_test)
```

__단계 4: 로지스틱 함수 구현__ 
---
##### 아래처럼 정의된 로지스틱 함수를 파이썬 함수로 구현한다. 

![alt text](/_image/img.PNG)

```python
def sigmoid(x):
    return 1/(1+np.exp(-x))
```
> ###### 나는 이 함수를 설정하는걸 중점으로 뒀다.<br>이 코드의 원본인 소프트맥스 코드를 응용해서 로지스틱 회귀를 구현할때 <br>	1.소프트맥스 함수를 로지스틱 함수로 변경<br>	2.소프트맥스 비용함수를 로지스틱 비용함수로 변경<br>
>
> ###### 이 두 가지였는데, <br>비용함수는 클래스를 가리키는 변수 K의 값이 2이면 로지스틱 회귀의 비용함수와 같기 때문에 변경하지않았고,<br>로지스틱 함수는 무조건 들어가야되기 때문에 1번사항만 변경했다.



__단계 5: 경사하강법 활용 훈련__ 
---

#### 소프트맥스의 비용함수에서 k가 2일 경우 로지스틱 회귀의 비용함수와 같기때문에 수정하지 않고 그대로 사용하였다.

> ###### 위에서 설명한 것과 같은 말이다.



##### 이제 훈련 코드를 작성할 수 있다.
##### 클래스별 파라미터로 이루어진 (n+1, K) 행렬 모양의 2차원 넘파이 어레이 Theta를 생성하기 위해 n과 K를 확인한다.

```python
n_inputs = X_train.shape[1]           # 특성 수(n) + 1, 붓꽃의 경우: 특성 2개 + 1
n_outputs = len(np.unique(y_train))   # 중복을 제거한 클래스 수(K), 붓꽃의 경우: 3개
```

```
n_inputs

```
=> 2

```
n_outputs

```
=> 2

##### 파라미터 Theta를 무작위로 초기 설정한다.

```python
Theta = np.random.randn(n_inputs, n_outputs)
```

> ###### 이제 훈련을 시키기 위해 처음으로 초기화시켰다.



##### 배치 경사하강법 훈련은 아래 코드를 통해 이루어진다.


- `eta = 0.01`: 학습률
- `n_iterations = 5001` : 에포크 수
- `m = len(X_train)`: 훈련 세트 크기, 즉 훈련 샘플 수
- `epsilon = 1e-7`: $\log$ 값이 항상 계산되도록 더해지는 작은 실수
- `logits`: 모든 샘플에 대한 클래스별 점수, 즉 $\mathbf{X}_{\textit{train}}\, \Theta$
- `Y_proba`: 모든 샘플에 대해 계산된 이진분류 확률, 즉 $\hat P$

```python
# 배치 경사하강법 구현

eta = 0.01
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7

for iteration in range(n_iterations):     # 5001번 반복 훈련
    logits = X_train.dot(Theta)
    Y_proba = sigmoid(logits)
    
    if iteration % 500 == 0:              # 500 에포크마다 손실(비용) 계산해서 출력
        loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))
        print(iteration, loss)
    
    error = Y_proba - Y_train_one_hot     # 그레이디언트 계산.
    gradients = 1/m * X_train.T.dot(error)
    
    Theta = Theta - eta * gradients       # 파라미터 업데이트
```

```
=>  0 0.6216585388989472
	500 0.5358155407854331
	1000 0.46868915858417054
	1500 0.4228110723139094
	2000 0.389997119276909
	2500 0.36532453055572167
	3000 0.34600634309210915
	3500 0.33038385603797926
	4000 0.3174181616594949
	4500 0.3064286658965357
	5000 0.2969519170535297
```

> ###### 손실값이 점점 내려가는게 보인다.<br>이 결과값만 본다면 좋은 방향으로 훈련되고 있다고 생각된다.



##### 학습된 파라미터는 다음과 같다.

```python
Theta
```

```
=> array([[ 3.49367474, -3.54330311],
          [-2.15880998,  2.18878276]])
```



##### 검증 세트에 대한 예측과 정확도는 다음과 같다.<br>`logits`, `Y_proba`를 검증 세트인 `X_valid`를 이용하여 계산한다.<br>예측 클래스는 `Y_proba`에서 가장 큰 값을 갖는 인덱스로 선택한다.
```python
logits = X_valid.dot(Theta)              
Y_proba = sigmoid(logits)
y_predict = np.argmax(Y_proba, axis=1)          # 가장 높은 확률을 갖는 클래스 선택

accuracy_score = np.mean(y_predict == y_valid)  # 정확도 계산
accuracy_score
```

```
=> 0.9666666666666667
```

> ###### 정확도가 매우 높게 나왔다.<br>맨처음 결과가 이렇게 나왔을 때 이런 생각을 하며 매우 놀랐다.
>
> > ### 왜이리 높게 나오지? <br>아직 규제도 적용안했는데?<br>뭔가 잘못 작성했나?
>
> ###### 그래도 일단 오류가 발생한게 아니고 정상적으로 훈련이 되니 다음 단계로 넘어갔다.



__단계 6: 규제가 추가된 경사하강법 활용 훈련__ 
---

##### alpha = 규제 하이퍼파라미터

```python
eta = 0.1
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.1   # 규제 하이퍼파라미터

Theta = np.random.randn(n_inputs, n_outputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    logits = X_train.dot(Theta)
    Y_proba = sigmoid(logits)
    
    if iteration % 500 == 0:
        xentropy_loss = -np.mean(np.sum(Y_train_one_hot * np.log(Y_proba + epsilon), axis=1))
        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))  # 편향은 규제에서 제외
        loss = xentropy_loss + alpha * l2_loss        # l2 규제가 추가된 손실
        print(iteration, loss)
    
    error = Y_proba - Y_train_one_hot
    l2_loss_gradients = np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]   # l2 규제 그레이디언트
    gradients = 1/m * X_train.T.dot(error) + l2_loss_gradients
    
    Theta = Theta - eta * gradients

logits = X_valid.dot(Theta)
Y_proba = sigmoid(logits)
y_predict = np.argmax(Y_proba, axis=1)

accuracy_score = np.mean(y_predict == y_valid)
accuracy_score
```

```
=>  0 0.7162610260856582
	500 0.5424422902989774
	1000 0.5489288755349917
	1500 0.549640024455795
	2000 0.5497072521674562
	2500 0.5497135348935831
	3000 0.5497141214229124
	3500 0.5497141761734805
	4000 0.5497141812842178
	4500 0.5497141817612832
	5000 0.5497141818058153
```

> ###### 규제를 적용했더니 손실값이 규제 적용전보다 커졌고, 어느 특수한 값에 수렴하게됬다.<br>규제를 적용한게 더 않좋다는 결과를 보면서 
>
> > ###### 규제가 그렇게 강하게 건것도 아닌데 이렇게 차이가 날수 있나?<br>규제에 민감하게 반응하는 것 같다.
>
> ###### 그래서 정확도를 높여야 되기 때문에 규제를 더 낮추는 방향으로 잡았다.



__단계 7: 조기 종료 추가__
---



##### 위 규제가 사용된 모델의 훈련 과정에서<br>매 에포크마다 검증 세트에 대한 손실을 계산하여 오차가 줄어들다가 증가하기 시작할 때 멈추도록 한다.
> ###### 이전 단계에서 규제를 적용하였을때 정확도가 떨어졌으니 규제를 0.01로 약하게 정하였다.

```python
eta = 0.1 
n_iterations = 5001
m = len(X_train)
epsilon = 1e-7
alpha = 0.01              # 규제 하이퍼파라미터
best_loss = np.infty   # 최소 손실값 기억 변수

Theta = np.random.randn(n_inputs, n_outputs)  # 파라미터 새로 초기화

for iteration in range(n_iterations):
    # 훈련 및 손실 계산
    logits = X_train.dot(Theta)
    Y_proba = sigmoid(logits)
    error = Y_proba - Y_train_one_hot
    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]
    Theta = Theta - eta * gradients

    # 검증 세트에 대한 손실 계산
    logits = X_valid.dot(Theta)
    Y_proba = sigmoid(logits)
    xentropy_loss = -np.mean(np.sum(Y_valid_one_hot * np.log(Y_proba + epsilon), axis=1))
    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))
    loss = xentropy_loss + alpha * l2_loss
    
    # 500 에포크마다 검증 세트에 대한 손실 출력
    if iteration % 500 == 0:
        print(iteration, loss)
        
    # 에포크마다 최소 손실값 업데이트
    if loss < best_loss:
        best_loss = loss
    else:                                      # 에포크가 줄어들지 않으면 바로 훈련 종료
        print(iteration - 1, best_loss)        # 종료되지 이전 에포크의 손실값 출력
        print(iteration, loss, "조기 종료!")
        break

logits = X_valid.dot(Theta)
Y_proba = sigmoid(logits)
y_predict = np.argmax(Y_proba, axis=1)

accuracy_score = np.mean(y_predict == y_valid)
accuracy_score
```

```
=>	0 0.7456585550574103
	500 0.3388041580108214
	1000 0.3201659340346895
	1222 0.3193338407410206
	1223 0.31933385042637197 조기 종료!
```

```
logits = X_valid.dot(Theta)
Y_proba = sigmoid(logits)
y_predict = np.argmax(Y_proba, axis=1)

accuracy_score = np.mean(y_predict == y_valid)
accuracy_score
```

```
=> 0.9666666666666667
```

> ###### 과제 5이전에 과제 4 해결중에는 규제전이 0.9였고 규제이후가 0.96으로 알고있었는데<br>잘못 알고있었던 것 같다. 이러면 사실상 규제를 적용하는게 큰 의미가 있는지 싶다.



## ~~과제 2~~

~~과제 1에서 구현된 로지스틱 회귀 알고리즘에 일대다(OvR) 방식을 적용하여 붓꽃에 대한 다중 클래스 분류 알고리즘을 구현하라. 단, 사이킷런을 전혀 사용하지 않아야 한다.~~

> #### 못했다<br>과제2를 구상하는 과정에서 고민하고, 헷갈려서 결국엔 손을 못댄 이유가 뭔가 하면,<br>
>
> > ​	다중클래스 분류 과정을 만든다 했을때
> >
> > 1. 세토사를 이진분류하는 모델, 버니지카를 분류하는 모델, 버지컬러를 분류하는 모델을 파이프라인으로 만듬
> > 2. 샘플에 대해 각 클래스 분류 모델 별로 분류를 한 값들을 서로 비교시켜  가장 높은 값을 가진 클래스를 선택
> > 3. 반복해서 전체 샘플에 대해 분류를 마침
>
> ###### 이런 과정을 생각했었는데 여기서 헷갈린게<br>
>
> - 경사하강법으로 샘플 하나를 예측했을때  [A , B]란 값이 나오는데, 이 값을 어떻게 다른 두개의 파이프라인 예측값과 비교를 시키는가?
> - 파이프라인으로 만들어서 분류를 시킨다했을때, 데이터 전처리는 파이프라인 밖에서 하는지, 아니면 각각의 파이프라인 안에서 전처리를 시키는가?
> - 전처리를 파이프라인 안에서 한다면, 각 파이프라인에 있는 데이터들은 같은 데이터들이 아니라 서로 다른 데이터가 되는게 아닌가?
> - 등등...
>
> ###### 같은 생각들을 갖고 고민하다 결국에는 저 과정을 구현하는데 한계가 있어 해결하지 못했다.





## ~~과제 3~~

~~A. 사진을 낮과 밤으로 분류하는 로지스틱 회귀 모델을 구현하라.~~

~~B. 사진을 낮과 밤, 실내와 실외로 분류하는 다중 레이블 분류 모델을 두 개의 로지스틱 회귀 모델을 이용하여 구현하라.~~

~~단, 모델 구현에 필요한 사진을 직접 구해야 한다. 최소 100장 이상의 사진 활용할 것.~~



> ## 해결하지 못한 이유
>
> - ###### 처음 생각했을 때 특성값을 모델을 훈련시키는 과정에서 추가시키는 것으로 생각을 해 난감했다.
>
>   - 지금와서 생각하면 그냥 내가 직접 추가하면 됬을것 같다.
>
> - ###### 사진을 보고 어떻게 낮과 밤, 실내와 실외를 분류시킬지, 기준을 어떻게 잡아야하는지에 대해 애매하게 느껴졌다.
>
>   - 수치같은걸 따로 잡아줘야 하는건가? 이런 생각을 했다.
>
> - ###### 등등...





# 느낀점과 소감

모델 코드를 보거나 실행만 시켰을때는 이해되는것처럼 느껴졌는데, 막상 내가 세세하게 설정하고 구현하려니<br>매우 어렵다. 역시 만만치 않은거같다.

